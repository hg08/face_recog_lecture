## 举例

​	给计算机输入一套房的面积，位置，朝向，房间数目，计算机就可以自动给你算出它的**价格**（回归算法）；输入一个人的学历，住址，朋友数目，去过的地方，计算机也可以自动给你算出他／她的**年收入**(回归算法)；输入一种植物的花瓣数目，花瓣宽度／长度，叶子长度/宽度，花香描述等，计算机就可以告诉我们这种植物的**名称**(分类算法)；......

​	这些问题都可以通过选择一个机器学习算法,给它数据,然后等待输出结果.　人脸识别也是可以通过机器学习算法来做的，但人脸识别(face recognition)是由一系列机器学习算法构成的．(显示同一个名人的三张照片)

1. 在图上找出所有的人脸．

2.  对同一张脸，即使它旋转或者扭曲，让计算机要能知道这是同样一张脸．

3.  能找出这张脸独特的特征，以用来将这张脸与其他人的脸区分开（特征可以是：眼睛大小，鼻子长度等）.

4. 将该脸的独特特征与所有已经标识的人脸进行对照,以确定该人脸对应的人的姓名.


​	注意：我们的大脑能瞬间完成上面所有的步骤！我们的大脑非常善于人脸识别！

## 人脸识别步骤

​	我们分四个步骤来处理人脸识别. 　对每一步, 我们将学习一个不同的机器学习算法．我们将会学习每一个算法背后的基本思想．以后你可以用Python去建立自己的人脸识别系统．可能用到的库有OpenFace和dlib.

### 第一步: 找出所有的人脸　

​	第一步是人脸探测. 　区分人脸之前，首先要找出人脸在照片中的位置!　近几年来的相机都有人脸探测的功能，它能找出人脸，以便可以对每张人脸对焦，从而得出更清晰的照片．我们这里用来做**识别人脸**，而非得到更清晰的照片．

​	注：自从Paul Viola和Michael Jones 发明了能应用在一般照相机上的快速探测人脸的方法，人脸探测在上个时代初就成为主流技术． 现在，人们有了更好的方法用于快速探测人脸． 

​	我们将用的方法----方向梯度直方图(Histogram of Oriented Gradients, HOG, 2005).　首先将照片转成黑白照片,因为色彩的信息我们不需要．然后，我们依次关注照片中的每一个像素点． 对每个像素点,　我们想看看那些直接包围着它的像素点. 我们的目标：计算出当前像素相对于其周围的像素有多暗. 然后我们想画出像素由明到暗变化最快的方向:

<img src="https://github.com/hg08/tututu/blob/master/oriented_gradients.gif?raw=true">



对照片中的**每一个像素点**重复上述操作, 最终每一个像素都被一个箭头取代了. 这些箭头称为**梯度** ,它们显示整张照片由明到暗变化最快的方向．

​	这样做有什么好处？ 如果我们直接分析像素, 同一个人的非常暗的照片和非常亮的照片将具有完全不同的像素值. 但是考虑亮度改变的**方向**时，暗照片和两照片将有几乎一样的表示! 所以我们不直接处理像素值信息，而是处理像素明暗的变化信息．

​	进一步简化：我们用梯度来看一些基本模式，而不是所有细节！为了达到这目的，我们将照片分成小方格，每个方格的大小为16x16像素． 在每一个小方格中,我们将计算在每个主要方向有多少个梯度点 (有多少个指向上方，有多少个指向下方，有多少个指向右上方，等等). 然后我们以最强的箭头方向去取代该小方格． 这样，我们将原图片转化成了非常简单的表示． 它只以简单的方式描述人脸的基本结构:

<img src="https://github.com/hg08/tututu/blob/master/basic_structure_of_face.png?raw=true" width="400">

​	为了在这张HOG图片中找到人脸, 我们只需找到我们的图片中最像**已知HOG 模式**的那部分．已知HOG模式由大量其他照片训练并提取出来，如图：

<img src="https://github.com/hg08/tututu/blob/master/HOG_face_pattern_standard.png?raw=true">



具体组成部分：运用Dlib库中的face detection algorithm 来看是否图片中有人脸存在. 如果有, 这算法会为每一张脸创建一个 "end position".

### 用scikit-image显示HOG图
计算 HOG的步骤如下：

	(optional) global image normalisation
	computing the gradient image in x and y
	computing gradient histograms
	normalising across blocks
	flattening into a feature vector

第一步 applies an optional global image normalisation equalisation that is designed to reduce the influence of illumination effects. In practice we use gamma (power law) compression, either computing the square root or the log of each color channel. Image texture strength is typically proportional to the local surface illumination so this compression helps to reduce the effects of local shadowing and illumination variations.

第二步 计算图片的一阶梯度. These capture contour, silhouette and some texture information, while providing further resistance to illumination variations. The locally dominant color channel is used, which provides color invariance to a large extent. Variant methods may also include second order image derivatives, which act as primitive bar detectors - a useful feature for capturing, e.g. bar like structures in bicycles and limbs in humans.

第三步 aims to produce an encoding that is sensitive to local image content while remaining resistant to small changes in pose or appearance. The adopted method pools gradient orientation information locally in the same way as the SIFT [2] feature. The image window is divided into small spatial regions, called “cells”. For each cell we accumulate a local 1-D histogram of gradient or edge orientations over all the pixels in the cell. This combined cell-level 1-D histogram forms the basic “orientation histogram” representation. Each orientation histogram divides the gradient angle range into a fixed number of predetermined bins. The gradient magnitudes of the pixels in the cell are used to vote into the orientation histogram.

第四步　computes normalisation, which takes local groups of cells and contrast normalises their overall responses before passing to next stage. Normalisation introduces better invariance to illumination（光照）, shadowing, and edge contrast. It is performed by accumulating a measure of local histogram “energy” over local groups of cells that we call “blocks”. The result is used to normalise each cell in the block. Typically each individual cell is shared between several blocks, but its normalisations are block dependent and thus different. The cell thus appears several times in the final output vector with different normalisations. This may seem redundant but it improves the performance. We refer to the normalised block descriptors as Histogram of Oriented Gradient (HOG) descriptors.

The final step collects the HOG descriptors from all blocks of a dense overlapping grid of blocks covering the detection window into a combined feature vector for use in the window classifier.

References
[1]	Dalal, N. and Triggs, B., “Histograms of Oriented Gradients for Human Detection,” IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005, San Diego, CA, USA.
[2]	David G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.

python 代码示例：

```python
# To calculate HOG and display the HOG image
import matplotlib.pyplot as plt
from skimage.feature import hog
from skimage import data, exposure
from skimage import color

image = color.rgb2gray(data.astronaut()) #to change to gray, after importing color
#image = data.astronaut()

fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),
                cells_per_block=(1, 1), visualise=True)
#fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),
#                    cells_per_block=(1, 1), visualise=True,　multichannel=True)
# use visulaize or visualise is OK, but for this version of skimage, use 'visualise'.

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)
ax1.axis('off')
ax1.imshow(image, cmap=plt.cm.gray)
ax1.set_title('Input image')

# Rescale histogram for better display
hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))

ax2.axis('off')
ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)
ax2.set_title('Histogram of Oriented Gradients')
plt.show()
```

### 方向梯度直方图


#### 什么是特征描述器(Feature Descriptor)
特征描述器是图像的一种表示 ，它用于提取有用信息，并抛掉额外的信息．

Typically, a feature descriptor 将具有尺寸为宽$\times$ 高 $\times$3 (channels )的图片转换为一个长度为$n$的特征矢量/数组. In the case of在HOG feature descriptor, the input image is of size 64 x 128 x 3 and the output feature vector is of length 3780.

Keep in mind that HOG descriptor can be calculated for other sizes, but in this post I am sticking to numbers presented in the original paper so you can easily understand the concept with one concrete example.

　　This all sounds good, but what is “useful” and what is “extraneous” ? To define “useful”, we need to know what is it “useful” for ? Clearly, the feature vector is not useful for the purpose of viewing the image. But, it is very useful for tasks like image recognition and object detection. The feature vector produced by these algorithms when fed into an image classification algorithms like Support Vector Machine (SVM) produce good results.

　　但是,对分类任务而言，什么样的"特征"是有用的呢？ 让我们举例讨论这一点. 假设我们想to build an object detector that detects buttons of shirts and coats. A button is circular ( may look elliptical in an image ) and usually has a few holes for sewing. You can run an edge detector on the image of a button, and easily tell if it is a button by simply looking at the edge image alone. In this case, edge information is “useful” and color information is not. In addition, the features also need to have discriminative（有识别力的） power. For example, good features extracted from an image should be able to tell the difference between buttons and other circular objects like coins and car tires.
　　
　　在 HOG 特征描述器中, 梯度之方向的分布( histograms 直方图)  ( oriented gradients ) 被用作特征. 一张图片的梯度(复)( x and y derivatives )很有用，因为在边沿和尖角周围梯度的数值非常大  ( regions of abrupt intensity changes )，且边沿和尖角比平坦区域包含更多的关于物体形状的信息.

#### 如何计算HOG(**)?
##### 处理
　As mentioned earlier HOG feature descriptor used for pedestrian detection is calculated on a 64×128 patch of an image. Of course, an image may be of any size. Typically patches at multiple scales are analyzed at many image locations. The only constraint is that the patches being analyzed have a fixed aspect ratio. In our case, the patches need to have an aspect ratio of 1:2. For example, they can be 100×200, 128×256, or 1000×2000 but not 101×205.

To illustrate this point I have shown a large image of size 720×475. We have selected a patch of size 100×200 for calculating our HOG feature descriptor. This patch is cropped out of an image and resized to 64×128. Now we are ready to calculate the HOG descriptor for this image patch. 

注意：　The paper by Dalal and Triggs also mentions gamma correction as a preprocessing step, but the performance gains are minor and so we are skipping the step.

##### 计算梯度图像
  为了计算一个HOG descriptor, 我们首先需要计算水平梯度和竖直梯度; after all, we want to calculate the histogram of gradients. 这可以由如下的核过滤该图片而得．
<img src='https://www.learnopencv.com/wp-content/uploads/2016/11/gradient-kernels.jpg'>

​	我们也可用OpenCV中的Sobel算符(with kernel size 1)来达到同样的目的．

```python
# Python gradient calculation 

# Read image
im = cv2.imread('li.jpg')
im = np.float32(im) / 255.0

# Calculate gradient 
gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)
gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)
```
然后，我们可以用$g_x$, $g_y$来计算梯度的大小和方向：
$$
\begin{eqnarray}
g &=& \sqrt{g_x^2 + g_y^2} \\
\theta &=& \text{arctan}\frac{g_y}{g_x}
\end{eqnarray}
$$

如果你用OpenCV, the calculation can be done using the function cartToPolar as shown below.
	
```python
# Python Calculate gradient magnitude and direction ( in degrees ) 
mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)
```





## 第二步: 人脸标记和变换

​	基本思想：我们将选出每张人脸上的68个特殊点 (*地标*) ----如眼睛的外边界, 眉毛的内边界,等. 然后我们将运用一个机器学习算法在任意一张人脸上找出这些特殊点:

​	当我们知道人脸的双眼和嘴的位置，我们将简单地旋转,伸缩和剪切(shear)这张图片，使得双眼和嘴尽可能地centered. 我们将不会做任何３维 warps以免引入照片的畸变. 我们只运用基本的图形变换，如保持平行线仍然平行的旋转变换和伸缩变换 (仿射变换(affine transformations))．

​	注：现在无论人脸如何转动, 我们都能将眼睛和嘴基本上置于照片中心相同的位置. 这将使我们下一步更精确.



## 第三步: 编码

​	现在开始区分不同的人脸!	最简单的人脸识别方法是直接将第二步中找出的未知人脸与我们已经带有标识的所有照片进行对照. 当我们发现一张已带有标签的人脸与未知人脸非常相似时,那它们一定是同一个人的脸. 这个想法有趣吧？

​	这个方法还有一个问题. 具有上亿用户和照片的网站不可能遍历每一张有标签的人脸来与新上传的照片进行对比.　那样太耗时.　人脸识别应该在数毫秒内完成．

​	我们需要的是一种用于提取每一张人脸的几个基本测量值的方法．然后我们可以以同样的方式测量新的人脸照片，并找出与之有最接近的测量值的已知人脸.　例如，我们可能测量每一只耳朵的尺寸,两眼间距, 鼻子长度等. 



### 测量人脸的最可靠方法

 我们应该从每一张脸上收集哪些测量值来建立我们的＂熟人＂数据库呢？　耳朵大小？眉毛长度?　眼睛大小？鼻子宽度？　人们发现，最精确的方法是**让计算机自己去决定该收集哪些测量值**．对于人脸的哪部分对测量而言最重要这一问题，深度学习算法比人类做得更好!　具体的解决办法是训练一个深度卷积神经网络, 并训练它对每张人脸产生128个测量值.

$$
2^7 = 128
$$

### 训练过程的工作方式

训练过程通过同时看３张人脸图片来工作：

1. 加载一个熟人的训练人脸照片(左)；
2. 加载同一个熟人的另一张照片(中)；
3. 加载另外一个人的一张照片(右)．


然后这个算法looks at这些三张照片中在当前产生的各自测量值. 然后，算法轻微地调整神经网络以确保照片1和照片2的测量值微微接近，而使得照片２和照片3的测量值微微远离:

<img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/train/WuYifan/wu1.jpg?raw=true" width="225"> <img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/train/WuYifan/wu2.png?raw=true" width="160">   <img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/test/li1.png?raw=true" width="185">

　　　　　照片１　　　　　　　　　照片２　　　　　　　　　照片３　　　　　　　　　

对数千人的数百万张照片重复这个过程数百万次,这个神经网络学会了可靠地对每一个人产生这128个测量值. 同一个人的任意数十张不同照片将会给出基本相同的测量值! 在机器学习领域，人们称每张人脸中的这128个测量值为 一个＂嵌入＂(embedding).  将图片这样的复杂数据简化为数字的列表这样的思想在机器学习领域常常出现．我们这里用的方法是2015年谷歌的研究人员发明的. 相似的方法还有很多.

### 为人脸编码

​	训练神经网络非常耗时，即使你有大型计算机．但是一旦神经网络训练完成，它便可以对任意一张从未见过的人脸产生出测量值．所以，训练只需要运行一次！  OpenFace的研究人员已经训练除了一些神经网络，我们可以直接使用 (参考Brandon Amos and team)! 我们需要亲自做的就是：使我们的人脸照片输入至他们已经训练好的神经网络中以得到那128个测量值. (measurements for our test image示例:)

如果想问这128个数值分别测量的是什么呢? 答案是我们不知道！我们关注的是这个网络在看同一个人的两张不同的照片时，产出出几乎一样的数.

​	注:　If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file (Comma-separated values). You run it like this.

## 第四步: 由编码找出人名

最后一步！我们只需在已标识的＂熟人＂数据集里，找出那个与测试照片中的人脸距离最近的人.

方法：任意基本的机器学习分类算法．无需深度学习算法．可以运用**线性SVM分类器**,　注意很多其他分类器也可以.

我们只需训练一个分类器 来测量新的测试照片，并给出哪一个认识的人与测试照片中人脸的距离最近． 运行这个分类器只花费几毫秒.　这个分类器的结果是这个人的名字． 

So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:

接下来，我运行分类器于Will Ferrell 和Chad Smith视频的每一帧，他们在Jimmy Fallon的电视节目上乔装成对方:

## 运行

概述我们的步骤:

运用HOG算法进行编码，以创建图片的简化版.

运用此简化图片,找出照片中最像一般的人脸HOG编码的部分.
通过找出人脸中主要的landmarks,计算出该人脸的pose.

一旦我们发现这些landmarks, 运用它们来warp该简化图片以使得双眼和嘴是centered.
输入该centered 人脸图片给一个神经网络，该神经网络知道如何测量该脸的特征. 保存这128个测量值.
观察我们过去已经测量过的所有人脸, 看哪一个人具有与我们当前人脸的测量值最接近的测量值. 找到该人脸，则匹配成功!



## 参考：

1. Adam Geitgey, Machine Learning is Fun!  Part 4: Modern Face Recognition with Deep Learning


2. Eorror: 

		ModuleNotFoundError: No module named 'cv2'

​       Solution:

	conda install -c conda-forge opencv
	conda install -c conda-forge/label/broken opencv

3. Error:
		
		UnicodeDecodeError: 'ascii' codec can't decode byte

	solution:
		Don't decode/encode willy nilly(不要糊涂，杂乱地编码／解码) 
			Don't assume your strings are UTF-8 encoded(不要假设你的字符串是以UTF-8编码的)
			Try to convert strings to Unicode strings as soon as possible in your code(尽快将您的字符串转化成Unicode字符串)

Python 2.x - The Long Version
Without seeing the source it's difficult to know the root cause, so I'll have to speak generally.

UnicodeDecodeError:
	'ascii' codec can't decode byte 
generally happens when you try to convert a Python 2.x str that contains non-ASCII to a Unicode string without specifying the encoding of the original string.

In brief, Unicode strings are an entirely separate type of Python string that does not contain any encoding.　简而言之，Unicode字符串完全是另一种并不包含任何＂编码＂的Python字符串． They only hold Unicode point codes and therefore can hold any Unicode point from across the entire spectrum. Strings contain encoded text, beit UTF-8, UTF-16, ISO-8895-1, GBK, Big5 etc. Strings are decoded to Unicode and Unicodes are encoded to strings. 字符串被解码为Unicode,Unicode被编码为字符串． Files and text data are always transferred in encoded strings.

The Markdown module authors probably use unicode() (where the exception is thrown) as a quality gate to the rest of the code - it will convert ASCII or re-wrap existing Unicodes strings to a new Unicode string. The Markdown authors can't know the encoding of the incoming string so will rely on you to decode strings to Unicode strings before passing to Markdown.

Unicode strings can be declared in your code using the u prefix to strings. E.g.

	>>> my_u = u'my ünicôdé strįng'
	>>> type(my_u)
	<type 'unicode'>

Unicode strings may also come from file, databases and network modules. When this happens, you don't need to worry about the encoding.

It's good practice to form a Unicode sandwich in your code, where you decode all incoming data to Unicode strings, work with Unicodes, then encode to strs on the way out. This saves you from worrying about the encoding of strings in the middle of your code.

