## 举例

​	给计算机输入一套房的面积，位置，朝向，房间数目，计算机就可以自动给你算出它的**价格**（回归算法）；输入一个人的学历，住址，朋友数目，去过的地方，计算机也可以自动给你算出他／她的**年收入**(回归算法)；输入一种植物的花瓣数目，花瓣宽度／长度，叶子长度/宽度，花香描述等，计算机就可以告诉我们这种植物的**名称**(分类算法)；......

​	这些问题都可以通过选择一个机器学习算法,给它数据,然后等待输出结果.　人脸识别也是可以通过机器学习算法来做的，但人脸识别(face recognition)是由一系列机器学习算法构成的．(显示同一个名人的三张照片)

1. 在图上找出所有的人脸．

2.  对同一张脸，即使它旋转或者扭曲，让计算机要能知道这是同样一张脸．

3.  能找出这张脸独特的特征，以用来将这张脸与其他人的脸区分开（特征可以是：眼睛大小，鼻子长度等）.

4. 将该脸的独特特征与所有已经标识的人脸进行对照,以确定该人脸对应的人的姓名.


​	注意：我们的大脑能瞬间完成上面所有的步骤！我们的大脑非常善于人脸识别！

## 人脸识别步骤

​	我们分四个步骤来处理人脸识别. 

​	对每一步, 我们将学习一个不同的机器学习算法．我们将会学习每一个算法背后的基本思想．以后你可以用Python去建立自己的人脸识别系统．可能用到的库有OpenFace和dlib.

### 第一步: 找出所有的人脸　

​	第一步是人脸探测. 区分人脸之前，首先要找出人脸在照片中的位置!

近几年来的相机都有人脸探测的功能，它能找出人脸，以便可以对每张人脸对焦，从而得出更清晰的照片．

而我们这里用来做识别人脸，而非得到更清晰的照片．

​	注：自从Paul Viola和Michael Jones 发明了能应用在一般照相机上的快速探测人脸的方法，人脸探测在上个时代初就成为主流技术．但现在，人类又有了更好的方法用于快速探测人脸． 

​	我们将用的方法----方向梯度直方图(Histogram of Oriented Gradients, HOG, 2005).　首先将照片转成黑白照片,因为色彩的信息我们不需要．

然后，我们依次关注照片中的每一个像素点． 对每个像素点,　我们想看看那些直接包围着它的像素点，如图:



我们的目标：计算出当前像素相对于其周围的像素有多暗. 然后我们想画出像素由明到暗变化最快的方向:



对照片中的**每一个像素点**重复上述操作, 最终每一个像素都被一个箭头取代了. 这些箭头称为**梯度** ,它们显示整张照片的光流:

这样做有什么好处？ 如果我们直接分析像素, 同一个人的非常暗的照片和非常亮的照片将具有完全不同的像素值. 但是考虑亮度改变的方向时，暗照片和两照片将有几乎一样的表示!

我们用梯度来看一些基本模式，而不是所有细节！为了达到这目的，我们将照片分成小方格，每个方格的大小为16x16像素． 在每一个小方格中,我们将计算在每个主要方向有多少个梯度点 (有多少个指向上方，有多少个指向下方，有多少个指向右上方，等等). 然后我们以最强的箭头方向去取代该小方格.

结果，我们将原来的图片转化成了非常简单的表示．它只以简单的方式描述人脸的基本结构：



为了在这张HOG图片中找到人脸, 我们只需找到我们的图片中最像**已知HOG 模式**的那部分．已知HOG模式由大量其他照片训练并提取出来．



## 第二步: 人脸标记和变换

​	基本思想：我们将选出每张人脸上的68个特殊点 (*地标*) ----如眼睛的外边界, 眉毛的内边界,等. 然后我们将运用一个机器学习算法在任意一张人脸上找出这些特殊点:

​	当我们知道人脸的双眼和嘴的位置，我们将简单地旋转,伸缩和剪切(shear)这张图片，使得双眼和嘴尽可能地centered. 我们将不会做任何３维 warps以免引入照片的畸变. 我们只运用基本的图形变换，如保持平行线仍然平行的旋转变换和伸缩变换(放射变换affine transformations)．

​	注：现在无论人脸如何转动, 我们都能将眼睛和嘴基本上置于照片中心相同的位置. 这将使我们下一步更精确.



## 第三步: 编码

现在开始区分不同的人脸!

最简单的人脸识别方法是直接将第二步中找出的未知人脸与我们已经带有标识的所有照片进行对照. 当我们发现一张已带有标签的人脸与未知人脸非常相似时,那它们一定是同一个人的脸. Seems like a pretty good idea, right?

这个方法还存在一个大问题. 具有上亿用户和照片的网站can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.

我们需要的是一种用于提取每一张人脸的几个基本测量的方法．Then we could measure our unknown face the same way and find the known face with the closest measurements.　例如，我们可能测量每一只耳朵的尺寸,两眼间距, 鼻子长度等. 

The most reliable way to measure a face
Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?

 Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. 对于人脸的哪部分对测量而言最重要这一问题，深度学习算法比人类做得更好.

解决办法是训练一个深度卷积神经网络, 并训练它对每张脸产生128个测量.

The training process works by looking at 3 face images同时:

Load a training face image of a known person
Load another picture of the same known person
Load a picture of a totally different person
Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:

After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.

Machine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.

Encoding our face image
This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.

But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!

So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image:

So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. 我们关注的是这个网络在看同一个人的两张不同的照片时，产出出几乎一样的数.

If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file (Comma-separated values). You run it like this.

## 第四步: 由编码找出人名

最后一步很简单！我们只需在已标识的人构成的数据集里，找出那个与测试照片中的人脸距离最近的人.

方法：任意基本的机器学习分类算法．无需深度学习算法．可以运用线性SVM分类器,　注意很多其他分类器也可以.

我们只需训练一个分类器 来测量新的测试照片，并给出哪一个认识的人与测试照片中人脸的距离最近． 运行这个分类器只花费几毫秒.　这个分类器的结果是这个人的名字． 

So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:

接下来，我运行classifier于Will Ferrell 和Chad Smith视频的每一帧，他们在Jimmy Fallon的电视节目上乔装成对方:

## 运行

概述我们的步骤:

运用HOG算法进行编码，以创建图片的简化版.

运用此简化图片,找出照片中最像一般的人脸HOG编码的部分.
通过找出人脸中主要的landmarks,计算出该人脸的pose.

一旦我们发现这些landmarks, 运用它们来warp该简化图片以使得双眼和嘴是centered.
输入该centered 人脸图片给一个神经网络，该神经网络知道如何测量该脸的特征. 保存这128个测量值.
观察我们过去已经测量过的所有人脸, 看哪一个人具有与我们当前人脸的测量值最接近的测量值. 找到该人脸，则匹配成功!


参考：Adam Geitgey, Machine Learning is Fun!  Part 4: Modern Face Recognition with Deep Learning

Eorror: 
	ModuleNotFoundError: No module named 'cv2'

Solution:

	conda install -c conda-forge opencv
	conda install -c conda-forge/label/broken opencv
<img src="https://github.com/hg08/tututu/blob/master/basic_structure_of_face.png?raw=true" width="700">

