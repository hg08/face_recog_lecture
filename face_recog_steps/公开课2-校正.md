## 公开课2:  人脸校正(face alignment)和变换

  我们分四个步骤来处理人脸识别. 　对每一步, 我们将学习一个不同的机器学习算法．我们将会学习每一个算法背后的基本思想．以后你可以用Python去建立自己的人脸识别系统．可能用到的库有OpenFace和dlib.

第一步中, 我们可以找出所有的人脸.下面介绍后面的步骤. 第二步:人脸对齐和变换． 

​	基本思想：我们将选出每张人脸上的68个特殊点 (*地标*) ---- 如眼睛的外边界, 眉毛的内边界, 等. 然后我们将运用一个机器学习算法在任意一张人脸上找出这些特殊点:  

​	当我们知道人脸的双眼和嘴的位置，我们将简单地旋转, 伸缩和剪切(shear)这张图片, 使得双眼和嘴尽可能地centered. 我们将不会做任何３维 warps以免引入照片的畸变. 我们只运用基本的图形变换，如保持平行线仍然平行的旋转变换和伸缩变换 (仿射变换(affine transformations))．

​	注：现在无论人脸如何转动, 我们都能将眼睛和嘴基本上置于照片中心相同的位置. 这将使我们下一步更精确.

Vahid和Sullivan等人在2014发明的技术：

 We show how an ensemble of regression trees can be used to estimate the face’s landmark positions directly from a  **sparse subset ** of pixel intensities, achieving
super-realtime performance with high quality predictions.
We present a general framework based on **gradient boosting** for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing
or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies
and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore
the effect of data augmentation using synthesized data.

引言：
　In this paper we present a new algorithm that performs face alignment in milliseconds and achieves accuracy superior or comparable to state-of-the-art methods on standard
datasets. The speed gains over previous methods is a consequence of identifying the essential components of prior face alignment algorithms and then incorporating them in
a streamlined formulation into a cascade of high capacity regression functions learnt via gradient boosting. 

　We show, as others have [8, 2], that face alignment can be solved with a cascade of regression functions. In our case　each regression function in the cascade efficiently estimates　the shape from an initial estimate and the intensities of a
sparse set of pixels indexed relative to this initial estimate.
Our work builds on the large amount of research over the　last decade that has resulted in significant progress for face alignment [9, 4, 13, 7, 15, 1, 16, 18, 3, 6, 19]. In particular,
we incorporate into our learnt regression functions two key elements that are present in several of the successful algorithms cited and we detail these elements now

### Boosting方法
Boosting方法最初来源于分类问题，但也可以拓展到回归问题．
Boosting方法的动机: 是将许多弱分类器组合起来构成一个强大的分类器集体. 
由此，boosting方法与bagging方法以及其他基于＂分类器集体＂的方法相似．
但是这些联系是表面的，boosting方法和它们有根本的不同．

最流行的boosting算法(Freund and Schapire,  1997) :  “AdaBoost.M1”

举例，对二分类问题．分类器的错误率：
${\overline{\text{err}}} = \frac{1}{N} \sum_{i=1}^N I(y_i \neq G(x_i))$

其中，$y_i\in \{-1,1\}$, 分类器用$G(x_i)$表示．

概念: 弱分类器(weak classifier): 错误率仅仅比随机猜测略好一点的分类器．

boosting方法的目标: 相继应用弱分类器算法以不断地修改数据的版本,
因此产生一系列分类器 $G_m(x), m = 1, 2, . . . , M$ .














https://blog.csdn.net/yanyan_xixi/article/details/36372901



opencv 仿射变换 根据眼睛坐标进行人脸对齐 计算变换后对应坐标.

<img src='http://www.mcudiy.cn/attachment/Mon_1407/33_8_eca362c3f10fea4.jpg?29'>

人脸图像及68个面部关键点   



<img src="http://www.mcudiy.cn/attachment/Mon_1407/33_8_c1ecd47da715b3e.jpg?28">



   仿射变换后人脸图像及关键点

仿射变换将原坐标$(x, y)$变换为新坐标$(x', y')$的计算方法： 



<img src="http://www.mcudiy.cn/attachment/Mon_1407/33_8_c43f1f2156d6b3f.jpg?17">

通过上面的公式， 可计算出原图像经过变换后的新图像.



Opencv仿射变换函数warpAffine： 

### 第三步: 编码

​	现在开始区分不同的人脸!	最简单的人脸识别方法是直接将第二步中找出的未知人脸与我们已经带有标识的所有照片进行对照. 当我们发现一张已带有标签的人脸与未知人脸非常相似时,那它们一定是同一个人的脸. 这个想法有趣吧？

​	这个方法还有一个问题. 具有上亿用户和照片的网站不可能遍历每一张有标签的人脸来与新上传的照片进行对比.　那样太耗时.　人脸识别应该在数毫秒内完成．

​	我们需要的是一种用于提取每一张人脸的几个基本测量值的方法．然后我们可以以同样的方式测量新的人脸照片，并找出与之有最接近的测量值的已知人脸.　例如，我们可能测量每一只耳朵的尺寸,两眼间距, 鼻子长度等. 

```python
	def load_image_file(file, mode='RGB')
		"""
		加载一个图片文件(.jpg, .png, etc)到一个numpy列表
		:param file: image file name or file object to load
		:param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
		:return: image contents as numpy array
		"""
		...
```

函数 face_recognition.face_locations(). 
function face_locations in module face_recognition.api

```python
def face_locations(img, number_of_times_to_upsample=1, model='hog')
	"""
	returns an array of bounding boxes of human faces in a image
	:param img: 一张图片(作为一个numpy数组)
	:param number_of_times_to_upsample: 寻找人脸时对图片进行上采样的次数.此数值越高,则可以探测到越小的人脸. 
	:param model: 所用的探测模型. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
	:return: A list of tuples of found face locations in css (top, right, bottom, left) order
	"""
	...
```

#### 测量人脸的可靠方法

  我们应该从每一张脸上收集哪些测量值来建立我们的＂熟人＂数据库呢？　耳朵大小？眉毛长度?　眼睛大小？鼻子宽度？　人们发现，最精确的方法是**让计算机自己去决定该收集哪些测量值**．对于人脸的哪部分对测量而言最重要这一问题，深度学习算法比人类做得更好!　具体的解决办法是训练一个深度卷积神经网络, 并训练它对每张人脸产生128个测量值.

#### 为人脸编码

  即使有大型计算机, 训练神经网络也非常耗时．但是一旦神经网络训练完成，它便可以对任意一张从未见过的人脸产生出测量值．所以，训练只需要运行一次！  OpenFace的研究人员已经训练除了一些神经网络，我们可以直接使用 (参考Brandon Amos and team)! 我们需要亲自做的就是：使我们的人脸照片输入至他们已经训练好的神经网络中以得到那128个测量值. 

  问: 这128个数值分别测量的是什么呢?
我们不知道！我们关注的是这个网络在看同一个人的两张不同的照片时，产出出几乎一样的数.


对图片中的人脸编码, 函数 face_recognition.face_encodings()
模块face_recognition.api中的函数face_encodings .
```python
def face_encodings(face_image, known_face_locations=None, num_jitters=1)
	"""
	对给定的图片中的每一张人脸, 返回128-dimension人脸编码.
	:param face_image: 包含一张或多张人脸之图片(一个numpy数组)
	:param known_face_locations: 可选 - 已经认识的每张脸的bounding boxes.
	:param num_jitters: 计算编码时,re-sample的次数. 其值越高, 越精确, 但更慢 (i.e. 100 is 100x slower)
	:return: 128-dimensional人脸编码的列表(对图片中每一张脸都会生成这样一个列表)
	"""
	...
```
这个人脸编码的列表会加到特征列表$X$中, 同时把其所在的文件夹名(即人名)加入到列表$y$中. 这样,一条记录就产生了! 对每一个文件夹中的图片文件进行同样的操作,就可以对每一张图片中的人脸都得到一条记录,或者称为样本.

这就有了一个 $N$行$128+1=129$列的数据集. 其中$N$为有效照片的张数.



KNN分类器 neighbors.KNeighborsClassifier

 sklearn.neighbors.classification模块中的KNeighborsClassifier类 

```python
def class KNeighborsClassifier(sklearn.neighbors.base.NeighborsBase, 	sklearn.neighbors.base.KNeighb orsMixin, sklearn.neighbors.base.SupervisedIntegerMixin, sklearn.base.ClassifierMixin)

"""
	Classifier implementing the k-nearest neighbors vote.
	Read more in the :ref:`User Guide <classification>`.
	
	参数
	----------
	n_neighbors : int, optional (default = 5) Number of neighbors to use by default for :meth:`kneighbors` queries.

	weights : str or callable, optional (default = 'uniform') weight function used in prediction.  Possible values: 
		- 'uniform' : uniform weights.  All points in each neighborhood are weighted equally.
		- 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
		- [callable] : 一个由用户定义的用于接受距离们之数组的函数. 它返回由权重们构成的且与距离们之数组同样形状的数组.
	
	algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, 计算最近邻居们的一些可选算法:
		- 'ball_tree' will use :class:`BallTree`
        - 'kd_tree' will use :class:`KDTree`
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.
        
	注意: fitting on sparse input will override the setting of this parameter, using brute force.
    
	leaf_size : int, optional (默认值 30) 传给 BallTree 或 KDTree的叶子尺寸.  除了影响存储数所需的内存以外, 该参数能也影响构建和查询的速度(speed of the construction and query). 最优值因问题不同而各不同. 
	
	p : integer, optional (default = 2) Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
	
	metric : string or callable, 默认值 'minkowski' 
	用于树的距离测度(distance metric) . 默认测度为minkowski, 取p=2时,等价于标准的Euclidean测度. 参考DistanceMetric类,查看可用的测度的列表.
     
	metric_params : dict, optional (default = None) 测度函数的额外的关键字参数.
     
	n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. Doesn't affect :meth:`fit` method.
  
注:
    参考:ref:`Nearest Neighbors <neighbors>` 在线文档,关于``algorithm`` 和 ``leaf_size``的选择的讨论.
     
注意:
     如果邻居`k+1` 和 `k`,有相等的距离,但是不同的标签, 那么结果将依赖与训练数据的ordering.

各方法的定义:
      
    __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metri    108 c='minkowski', metric_params=None, n_jobs=1, **kwargs)
    构造方法,以__init__()表示, 用于构造对象. 
    
    predict(self, X)
    对给定的数据,预测类标签.
          
    参数
    ----------
    X : array-like, shape (n_query, n_features), 或者 shape (n_query, n_indexed) 如果 metric == 'precomputed'
    测试样本集.    
    
    返回
    ----------
    y : 数组, 其shape [n_samples] 或 [n_samples, n_outputs]
    每个数据样本的类标签.
  
    predict_proba(self, X)
    返回对测试数据集X估算出的几率.
        
    参数
    -----------
    X : array-like, shape (n_query, n_features),  or (n_query, n_indexed) if metric == 'precomputed'
    用于测试的样本.
    
    返回
    -------
    p : 数组的形状 = [n_samples, n_classes], 或者是几个这样的数组的一个列表,如果n_outputs > 1. 
    输入样本的"class probabilities". 类按字典式排序(lexicographic order).
    数据及其他属性的定义:
     
     	__abstractmethods__ = frozenset([])
      
    ----------------------------------------------------------------------
	从sklearn.base.BaseEstimator继承的方法:
    
    	__getstate__(self)
    	__repr__(self)
    	__setstate__(self, state)
      
    get_params(self, deep=True)
    获取该评估函数的参数.
          
         参数
         ----------
         deep : boolean, optional
             如果取True, 将返回该评估函数的参数,以及所包含的子对象(子对象本身亦为评估函数).
          
         返回
         -------
         params : mapping of string to any
         Parameter names mapped to their values.
      
      set_params(self, **params)
          设置评估函数(estimator)的参数.
          
          该方法对嵌套对象(nested objects)(例如pipelines)奏效,对简单的评估函数也奏效. 嵌套对象具有如下形式的参数
         ``<component>__<parameter>`` 
         因此,更新一个嵌套对象中的每一个分量是可能的.
         
          返回
          -------
          self 
    ---------------------------------------------------------------------
    从sklearn.base.BaseEstimator继承的数据描述器:
     
    	__dict__
        	dictionary for instance variables (if defined)
      
    	__weakref__
        	list of weak references to the object (if defined)
      
    ----------------------------------------------------------------------
	从sklearn.neighbors.base.KNeighborsMixin继承的方法:

	kneighbors(self, X=None, n_neighbors=None, return_distance=True)
    找出样本的k个邻居.
          
    返回 indices of and distances to the neighbors of each point.     
          参数
          ----------
          X : array-like, shape (n_query, n_features), or (n_query, n_indexed) if  metric == 'precomputed'
              一个或多个查询点.
              If not provided, neighbors of each indexed point are returned.
              In this case, the query point is not considered its own neighbor.
          
          n_neighbors : int
              邻居数to get (default is the value passed to the constructor).
    
          return_distance : boolean, optional. 默认值为True.
              如果该参数取值为False, 距离将不会被返回.
              
          返回
          -------
          dist : array
             Array representing the lengths to points, only present if return_distance=True
      
      ind : array
              最近的点的索引in the population matrix.
          
    kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')
    Computes the (weighted) graph of k-Neighbors for points in X
          
         参数
         ----------
         X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'
             The query point or points.
             If not provided, neighbors of each indexed point are returned.
             In this case, the query point is not considered its own neighbor.
          
         n_neighbors : int
             每个样本的邻居数.
             (default is value passed to the constructor).
          
         mode : {'connectivity', 'distance'}, optional
            返回之矩阵的类型: 'connectivity' 将返回连接性矩阵(connectivity matrix)with ones and zeros, in 'distance' 边缘是点之间的Euclidean距离.
          
         返回值
         -------
         A : CSR格式的稀疏矩阵, shape = [n_samples, n_samples_fit]
              n_samples_fit is the number of samples in the fitted data
              A[i, j] is assigned the weight of edge that connects i to j.
          
          参考
          --------
          NearestNeighbors.radius_neighbors_graph
      
    ----------------------------------------------------------------------
    从sklearn.neighbors.base.SupervisedIntegerMixin继承的方法:
      
    fit(self, X, y)
    以X作为特征,y作为目标值,训练该模型.
    
    参数
    ----------
    X : {array-like, sparse matrix, BallTree, KDTree}
    训练数据. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] 如果metric='precomputed'.
          
    y : {array-like, sparse matrix}
    具有 shape=[n_samples] or [n_samples, n_outputs] 的目标值
      
    ----------------------------------------------------------------------
    从sklearn.base.ClassifierMixin继承的方法:
      
    score(self, X, y, sample_weight=None)
         在给定的测试数据和标签上,返回平均精确度.
          
         在多标签分类中, 这是子集准确度(subset accuracy)
         which is a harsh metric 
         因为对每一个样本(记录)你需要:each label set be correctly predicted.
          
         参数
         ----------
         X : array-like, shape = (n_samples, n_features)
           测试样本集.
          
         y : array-like, shape = (n_samples) or (n_samples, n_outputs)
           X的真实标签.
         
         sample_weight : array-like, shape = [n_samples], optional
           样本的权重.
          
         返回
         -------
         score : float
             Mean accuracy of self.predict(X) wrt. y.

```


KNeighborsClassifier类的使用. 

例1. 训练集$\{X,y\}$.  本例展示数据集的特征的散点图.

```python
# encoding:utf-8
# 文件: ex_scattering.py
# 本例将具有两个特征的数据集作散点图,并用knn分类

import mglearn
import matplotlib.pyplot as plt
import numpy as np

# 生成数据集 
# X,y都表示为numpy数组,方便用mglearn作散点图
X = np.array([[1, 2],[1.5, 4], [2,3], [3,10], [3.4,8], [4, 9]])
y = np.array([1, 1, 1, 0, 0, 0])

# 对数据集作图
mglearn.discrete_scatter(X[:,0], X[:,1],y)
plt.legend(["Class 0", "Class 1"], loc=4)
plt.xlabel("Feature 1")
plt.ylabel("Teature 2")
print("X.shape:{}".format(X.shape))
plt.show()
```
运行:
	python ex_scattering.py

结果如下图: 

<img src="https://github.com/hg08/tututu/blob/master/fig_ex_scattering.png?raw=true">

训练和测试:

```python
>>> from sklearn.neighbors import KNeighborsClassifier
>>> X = [[1,2],[1.5,4],[2,3],[3,10],[3.4,8],[4,12]]
>>> y =[1,1,1,0,0,0]
>>> neigh = KNeighborsClassifier(n_neighbors=1) 	# 实例neigh
>>> neigh.fit(X,y) 				# fit()方法
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=1, p=2,
           weights='uniform')
>>> print(neigh.predict([[1.1,13]]))
[0]
>>> print(neigh.predict([[1.1,3]]))
[1]
>>> print(neigh.predict_proba([[0.9,4]]))
[[ 0.  1.]]
>>> print(neigh.predict_proba([[0.9,20]]))
[[ 1.  0.]]
```

例2. NearestNeighbors类对象的实例的kneighbors_graph()方法.

```
X : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric == 'precomputed'
         The query point or points.
         If not provided, neighbors of each indexed point are returned.
         In this case, the query point is not considered its own neighbor.

         n_neighbors : int
             每个样本的邻居数.
             (默认值:传给the constructor的值).
          
         mode : {'connectivity', 'distance'}, optional
            Type of returned matrix: 'connectivity' will return the
            connectivity matrix with ones and zeros, in 'distance' the
            edges are Euclidean distance between points.
          
         返回
         -------
         A : CSR格式的稀疏矩阵, shape = [n_samples, n_samples_fit]
              n_samples_fit is the number of samples in the fitted data
              A[i, j] is assigned the weight of edge that connects i to j.
```

```python
>>> X=[[1],[3],[0],[4]]
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',
         metric_params=None, n_jobs=1, n_neighbors=2, p=2, radius=1.0)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[ 1.,  0.,  1.,  0.],
       [ 0.,  1.,  0.,  1.],
       [ 1.,  0.,  1.,  0.],
       [ 0.,  1.,  0.,  1.]])

```

例3.  我们从一个表示我们的数据集的数组触发,我们构建一个NeighborsClassifier类, 并提问: 哪一个点离[1, 1, 0.5] 最近?

Python交互式代码如下:

```python
>>> from sklearn.neighbors import NearestNeighbors
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .8]]
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples) 
NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',
         metric_params=None, n_jobs=1, n_neighbors=1, p=2, radius=1.0)
>>> print(neigh.kneighbors([[1., 1., .5]]))
(array([[ 0.3]]), array([[2]]))
>>> 
>>> X = [[0., 1., 0.], [1., 0., .6]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
       [2]])
```

程序返回 [[0.3]]和 [[2]].  这意味着这个元素is at distance 0.3, 是第三个元素.(注意:从0开始计数). 



pickle.dump()
_pickle模块中的内置函数dump() . 可将对象的表示存入文件对象.

```python       
def dump(obj, file, protocol=None, *, fix_imports=True)
	"""
	Write a pickled representation of obj to the open file object file.  
    The optional *protocol* argument tells the pickler to use the given protocol supported protocols are 0, 1, 2, 3 and 4.  The default protocol is 3; a backward-incompatible protocol designed for Python 3.
    Specifying a negative protocol version selects the highest protocol version supported.  The higher the protocol used, the more recent the version of Python needed to read the pickle produced.
    The *file* argument must have a write() method that accepts a single
bytes argument.  It can thus be a file object opened for binary writing, a io.BytesIO instance, or any other custom object that meets this interface.
    If *fix_imports* is True and protocol is less than 3, pickle will try to map the new Python3 names to the old module names used in Python2, so that the pickle data stream is readable with Python 2.
	"""
	...
```

#### 训练过程的工作方式

训练过程通过同时看３张人脸图片来工作：

1. 加载一个熟人的训练人脸照片(左)；
2. 加载同一个熟人的另一张照片(中)；
3. 加载另外一个人的一张照片(右)．




<img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/train/WuYifan/wu1.jpg?raw=true" width="225"> <img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/train/WuYifan/wu2.png?raw=true" width="160">   <img src="https://github.com/hg08/face_recog_lecture/blob/master/knn_examples/test/li1.png?raw=true" width="185">

　　　　　照片１　　　　　　　 　 　照片２　　　　　　 　　　照片３　　

  然后, 这个算法考查这些三张照片中在当前产生的各自测量值. 然后，算法轻微地调整神经网络以确保照片1和照片2的测量值微微接近，而使得照片２和照片3的测量值微微远离:　　　　　　　

  对数千人的数百万张照片重复这个过程数百万次, 这个神经网络学会了可靠地对每一个人产生这128个测量值. 同一个人的任意数十张不同照片将会给出基本相同的测量值!  在机器学习领域，人们称每张人脸中的这128个测量值为 一个＂嵌入＂(embedding).  **将图片这样的复杂数据简化为由数字构成的列表**这样的思想在机器学习领域常常出现．我们这里用的方法是2015年谷歌的研究人员发明的.  相似的方法还有很多.




### 参考文献

[1]	Dalal, N. and Triggs, B., “Histograms of Oriented Gradients for Human Detection,” IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005, San Diego, CA, USA.

[2]	David G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.

[3]  Adam Geitgey, Machine Learning is Fun!  Part 4: Modern Face Recognition with Deep Learning

[4]  http://dlib.net/python/index.html 

[5]  Compressed Sparse Row（CSR）矩阵格式通过用三个一维的数组来存储一个m×n的矩阵$M$. 首先定义NNZ（Num-non-zero）为矩阵$M$中非0元素的个数. 例如, 矩阵$M$为:
$$
\begin{pmatrix}
  1 & 2 & 0 & 0 \\
  0 & 3 & 9 & 0 \\
  0 & 1 & 4 & 0
\end{pmatrix}
$$
那么，第一个数组为Val数组.  Val数组的长度为NNZ，分别为从左到右从上到下的非零元素值.

	val=[1 2 3 9 1 4]

第二个数组为Col_ind数组. Col_ind数组的长度为NNZ，其对应于Val数组中的元素的column_index（例如元素9排列在所在行的第3个位置, 因此其col_index为2）.

	col_ind=[0 1 1 2 1 2](从左到右，从上到下，该非零元素在的列)

第三个数组为Row_ptr.  row_ptr数组大小为$m+1$，其前m个元素分别代表这每一行中第一个非零元素在Val数组的下标. 例如，元素3是第二行的第一个非零元素，在Val数组中的下标为2.  row_ptr数组的含义表示了矩阵$M$的Row i， 其非零元素从Val[row_ptr[i]] to Val[row_ptr[i+1]-1]

	row_ptr=[0 2 4 6]

Row_ptr中最后一个元素等于NNZ. 因此，根据其意义，表示

第一行中的非零元素为Val[0] - Val[2-1]（1, 2）

第二行中的非零元素为Val[2] - Val[4-1]（3, 9）

第三行中的非零元素为Val[4] - Val[6-1]（1, 4）


注意，使用CSR存储方式后，所用内存反而增加了! 实际上，满足NNZ$<(m(n−1)−1)/2$时，才会节省内存空间.

Compressed Sparse Row的名字由来: CSR与COO相比，压缩了行（rowIndex）的信息. CSC和CSR类似，只不过和CSR行列互换。val[ ]数组里是按列存的数值，row_ptr变成了col_ptr，col_ind变成了row_ind。

注: SpMV是线性迭代解法器常用的计算内核之一。稀疏矩阵（Sparse Matrix）非零元的分布决定了SpMV实现时内存访问的格式，进而影响其运行效率。根据非零元的分布特点，有不同的存储格式（如COO，CSR，DOK，LIL，CSC，ELL等）.

[6]闵可夫斯基测度.  Minkowski测度的形式体系简介如下.

The present purpose is to show semi-rigorously how formally one may apply the Minkowski metric to two vectors and obtain a real number, i.e. 显示微分的角色, 以及它们如何在计算中消失. The setting is that of smooth manifold theory, and concepts such as convector fields and exterior derivatives are introduced.

A formal approach to the Minkowski metric
Minkowski测度(in coordinates as a tensor field on spacetime) 的一个成熟版本有如下形式:

$$
\eta _{\mu \nu }dx^{\mu }\otimes dx^{\nu }=\eta _{\mu \nu }dx^{\mu }\odot dx^{\nu }=\eta _{\mu \nu }dx^{\mu }dx^{\nu }.
$$

解释: 
The coordinate differentials are 一次形式场(1-form fields). 它们被定义为外微分(exterior derivative) of the coordinate functions $x_\mu$. These quantities evaluated at a point $p$ provide a basis for the cotangent space at $p$. The tensor product (denoted by the symbol $\otimes$) yields a tensor field of type $(0, 2)$, i.e. the type that expects two contravariant vectors as arguments. 在右边, the symmetric product (denoted by the symbol $odot$ or by juxtaposition) has been taken. The equality holds since, by definition, the Minkowski metric is symmetric. The notation on the far right is also sometimes used for the related, but different, line element. It is not a tensor. For elaboration on the differences and similarities, see Misner, Thorne & Wheeler (1973, Box 3.2 and section 13.2.)

Tangent vectors are, in this formalism, given in terms of a basis of differential operators of the first order,
$$
\left.{\frac {\partial }{\partial x^{\mu }}}\right|_{p},
$$
where $p$ is an event. This operator applied to a function $f$ gives the directional derivative of $f$ at $p$ in the direction of increasing xμ with xν, ν ≠ μ fixed. They provide a basis for the tangent space at p.

The exterior derivative $df$ of a function $f$ is a covector field, i.e. an assignment of a cotangent vector to each point p, by definition such that

$df(X)=Xf$, $df(X)=Xf$,
for each vector field X. A vector field is an assignment of a tangent vector to each point p. In coordinates X can be expanded at each point p in the basis given by the ∂/∂xν|p. Applying this with f = xμ, the coordinate function itself, and X = ∂/∂xν, called a coordinate vector field, one obtains

$dx^{\mu }\left({\frac {\partial }{\partial x^{\nu }}}\right)={\frac {\partial x^{\mu }}{\partial x^{\nu }}}=\delta _{\nu }^{\mu }$.
Since this relation holds at each point $p$, the dxμ|p provide a basis for the cotangent space at each $p$ and the bases dxμ|p and ∂/∂xν|p are dual to each other,

$ \left.dx^{\mu }\right|_{p}\left(\left.{\frac {\partial }{\partial x^{\nu }}}\right|_{p}\right)=\delta _{\nu }^{\mu }$.

at each $p$. Furthermore, one has

$ \alpha \otimes \beta (a,b)=\alpha (a)\beta (b)$ 

for general 1-forms on a tangent space α, β and general tangent vectors a, b. (This can be taken as a definition, but may also be proved in a more general setting.)

Thus when the metric tensor is fed two vectors fields $a, b$, both expanded in terms of the basis coordinate vector fields, the result is

 $\eta _{\mu \nu }dx^{\mu }\otimes dx^{\nu }(a,b)=\eta _{\mu \nu }a^{\mu }b^{\nu }$,

where aμ, bν are the component functions of the vector fields. The above equation holds at each point $p$, and the relation may as well be interpreted as the Minkowski metric at $p$ applied to two tangent vectors at $p$.

As mentioned, in a vector space, such as that modelling the spacetime of special relativity, tangent vectors can be canonically identified with vectors in the space itself, and vice versa. This means that the tangent spaces at each point are canonically identified with each other and with the vector space itself. This explains how the right hand side of the above equation can be employed directly, without regard to spacetime point the metric is to be evaluated and from where (which tangent space) the vectors come from.

This situation changes in general relativity. There one has

$g(p)_{\mu \nu }dx^{\mu }|_{p}dx^{\nu }|_{p}(a,b)=g(p)_{\mu \nu }a^{\mu }b^{\nu }$,

where now η → g(p), i.e. $g$ is still a metric tensor but now depending on spacetime and is a solution of Einstein's field equations. Moreover, $a, b$ must be tangent vectors at spacetime point $p$ and can no longer be moved around freely.

[7]  Axis-aligned minimum bounding box
一个给定点集的axis-aligned minimum bounding box (or AABB),  is its minimum bounding box subject to the constraint that the edges of the box are parallel to the (Cartesian) coordinate axes. 简单地说, 它就是$N$个intervals的Cartesian积. 每一个interval通过$S$中的点们的响应坐标的最小值和最大值来定义.

Axis-aligned minimal bounding boxes are used to an approximate location of an object in question and as a very simple descriptor of its shape. For example, in computational geometry and its applications when it is required to find intersections in the set of objects, the initial check is the intersections between their MBBs. Since it is usually a much less expensive operation than the check of the actual intersection (because it only requires comparisons of coordinates), it allows quickly excluding checks of the pairs that are far apart.

参考:  https://en.wikipedia.org/wiki/Minimum_bounding_box



[8] Example of Lagrange multipliers: Entropy. (wikipedia)

假设我们希望找到the discrete probability distribution on the points $\{p_{1},p_{2},\cdots ,p_{n}\}$  with maximal information entropy. This is the same as saying that we wish to find the least structured probability distribution on the points  $\{p_{1},p_{2},\cdots ,p_{n}\}$. In other words, we wish to maximize the Shannon entropy equation:  ($n$维概率空间)

$f(p_{1},p_{2},\cdots ,p_{n})=-\sum _{j=1}^{n}p_{j}\log _{2}p_{j}$.

For this to be a probability distribution the sum of the probabilities $p_{i}$ at each point $x_{i}$ must equal 1, so our constraint is:

$g(p_{1},p_{2},\cdots ,p_{n})=\sum _{j=1}^{n}p_{j}=1$.

We use Lagrange multipliers to find the point of maximum entropy, $ {\vec {p}}^{\,*}$ , across all discrete probability distributions ${\vec {p}}$  on $\{x_{1},x_{2},\cdots ,x_{n}\}$. We require that:

$\left.{\frac {\partial }{\partial {\vec {p}}}}(f+\lambda (g-1))\right|_{{\vec {p}}={\vec {p}}^{\,*}}=0$,

which gives a system of $n$ equations, $k={1,\cdots ,n}$, such that:

$\left.{\frac {\partial }{\partial p_{k}}}\left\{-\left(\sum _{j=1}^{n}p_{j}\log _{2}p_{j}\right)+\lambda \left(\sum _{j=1}^{n}p_{j}-1\right)\right\}\right|_{p_{k}=p_{k}^{*}}=0.$

Carrying out the differentiation of these n equations, we get

$ -\left({\frac {1}{\ln 2}}+\log _{2}p_{k}^{*}\right)+\lambda =0$.

This shows that all $p_{k}^{*}$ are equal (because they depend on λ only). By using the constraint

$\sum _{j}p_{j}=1$, we find

$p_{k}^{*}={\frac {1}{n}}$. 
Hence, the uniform distribution is the distribution with the greatest entropy, among distributions on n points.